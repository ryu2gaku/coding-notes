# 进程 Process

进程是资源分配的最小单位，也是线程的容器。程序是固定不变的，而进程会根据运算需要，让操作系统动态分配各种资源。

- 线程不能独立执行，必须依存在进程中
- 进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率
- 进程是资源分配的最小单位，线程是 CPU 调度的最小单位

## 1. 基本用法

`multiprocessing` 模块，通过创建 `Process` 对象并调用其 `start()` 方法来生成进程

```py
import multiprocessing


def cube(num):
    print(f"Cube: {num ** 3}")


def square(num):
    print(f"Square: {num ** 2}")


if __name__ == "__main__":
    p1 = multiprocessing.Process(target=square, args=(10,))
    p2 = multiprocessing.Process(target=cube, args=(10,))

    p1.start()
    p2.start()
    p1.join()
    p2.join()
    print("Done!")

# 结果:
# Square: 100
# Cube: 1000
# Done!
```

```py
"""
multiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)

创建一个进程

Process 与 Thread 的用法相似, 也有 start、run、join 等方法
Process 类适合简单的进程创建, 如需资源共享可以结合 multiprocessing.Queue 使用
如果想要控制进程数量, 则建议使用进程池 Pool 类

参数
group: 目前还未实现, 应始终为 None
target: 要调用的可调用对象
name: 进程名
args: 目标调用的参数元组
kwargs: 目标调用的关键字参数字典
daemon: 将进程 daemon 标志设置为 True 或 False
        如果是 None (默认), 则该标志将从创建的进程继承
"""
```

## 2. 常用操作

### 2.1. 获取当前进程对象

```py
"""
multiprocessing.current_process()

返回与当前进程相对应的 Process 对象

和 threading.current_thread() 相似
"""
```

### 2.2. 进程的名称和 ID

```py
from multiprocessing import Process
import os


def info(title):
    print('*' * 21)
    print(title)
    print('module name:', __name__)
    print('parent process:', os.getppid())
    print('process id:', os.getpid())
    print('*' * 21)


def f(name):
    info('function f')
    print('hello', name)


if __name__ == '__main__':
    info('main line')
    p = Process(target=f, args=('bob',))
    p.start()
    p.join()

# 结果:
# *********************
# main line
# module name: __main__
# parent process: 235
# process id: 1543
# *********************
# *********************
# function f
# module name: __main__
# parent process: 1543
# process id: 1544
# *********************
# hello bob
```

```py
"""
1. Process.name - 进程的名称

2. Process.pid  - 返回进程 ID, 在生成该进程之前为 None
3. os.getppid() - 返回当前进程 ID
4. os.getppid() - 返回父进程 ID
"""
```

### 2.3. 杀死进程

```py
"""
1. Process.terminate() - 终止进程

2. Process.is_alive()  - 判断进程是否还存活
"""
```

### 2.4. 守护进程 Daemon Process

默认情况下，在所有子进程退出之前，主程序不会退出。有些时候，启动后台进程运行而不阻止主程序退出是有用的，例如为监视工具生成“心跳”的任务。

```py
import multiprocessing as mp
import time


def f():
    name = mp.current_process().name
    print(f'启动 {name}')
    time.sleep(3)
    print(f'退出 {name}')


if __name__ == '__main__':
    background_process = mp.Process(name='background_process', target=f, daemon=True)
    NO_background_process = mp.Process(name='NO_background_process', target=f)
    background_process.start()
    NO_background_process.start()

# 结果:
# 启动 NO_background_process
# 退出 NO_background_process
```

## 3. 进程间通信

进程间默认是不能共享全局变量的（子进程不能改变主进程中全局变量的值）

`multiprocessing` 模块支持 Queue（队列）和 Pipe（管道）两种方式进行进程间通信。

### 3.1. 通过 Queue 实现进程间的通信

一个进程向 Queue 中放入数据，另一个进程从 Queue 中读取数据。

```py
from multiprocessing import Process, Queue


def f(q):
    q.put([42, None, 'hello'])


if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())
    p.join()

# 结果:
# [42, None, 'hello']
```

```py
"""
1. multiprocessing.Queue([maxsize])
返回一个进程共享队列

2. queue.qsize() - 返回队列的大致长度
注意, 在 Unix 平台上, 例如 Mac OS X, 这个方法可能会抛出 NotImplementedError 异常,
因为该平台没有实现 sem_getvalue()

3. queue.empty() - 队列是否为空
4. queue.full()  - 队列是否已满

5. queue.put(obj[, block[, timeout]]) - 将 obj 放入队列
6. queue.put_nowait(obj) - 相当于 put(obj, False)
仅当有可用缓冲槽时才放入对象, 否则抛出 queue.Full 异常

7. queue.get([block[, timeout]]) - 从队列中移除并返回一个项目
8. queue.get_nowait() - 相当于 get(False)
仅当有可用对象能够取出时返回, 否则抛出 queue.Empty 异常
"""

"""
multiprocessing 模块下的 Queue 和 queue 模块下的 Queue 基本类似,
都提供了 qsize() empty() full() put() put_nowait() get() get_nowait() 等方法

区别在 multiprocessing 模块下的 Queue 为进程提供服务,
而 queue 模块下的 Queue 为线程提供服务
"""
```

#### 3.1.1. Queue 案例

```py
import multiprocessing


def download_from_web(q):
    """下载数据"""
    # 模拟从网络下载来的数据
    data = [1, 2, 3, 4, 5, 6]
    # 向队列中写入数据
    for temp in data:
        q.put(temp)
    print('已经下载完数据, 并存入到队列中')


def analysis_data(q):
    """数据处理"""
    # 从队列中获取数据
    waitting_analysis_data = list()
    while True:
        data = q.get()
        waitting_analysis_data.append(data)

        if q.empty():
            break

    # 模拟数据处理
    print(waitting_analysis_data)


if __name__ == "__main__":
    # 创建一个队列
    q = multiprocessing.Queue()

    # 创建多个进程，将队列作为实参进行传递
    p1 = multiprocessing.Process(target=download_from_web, args=(q,))
    p2 = multiprocessing.Process(target=analysis_data, args=(q,))

    p1.start()
    p2.start()
    p1.join()
    p2.join()
```

### 3.2. 通过 Pipe 实现进程间的通信

Pipe 代表连接两个进程的管道。程序在调用 `Pipe()` 函数时会产生两个连接端，分别交给通信的两个进程，接下来进程既可从该连接端读取数据，也可向该连接端写入数据。

```py
from multiprocessing import Process, Pipe


def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()


if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())
    p.join()

# 结果:
# [42, None, 'hello']
```

```py
"""
1. multiprocessing.Pipe([duplex])

返回 (conn1, conn2), 两个 connection 对象代表管道两端
若 duplex 为 True (默认值), 则管道是双向的
若 duplex 为 False, 则管道是单向的, conn1 只能用于接收消息, conn2 只能用于发送消息

2. connection.send(obj)
3. connection.recv()
"""
```

## 4. 进程池

初始化 Pool 时，可以指定一个最大进程数，当有新的任务提交到 Pool 中时，如果池还没有满，就会创建一个新的进程用来执行该任务。但如果池中的进程数已经达到指定的最大值，那么该任务就会阻塞，直到有进程执行任务结束，才执行新的任务。

```py
import multiprocessing
import time
import os
import random


def worker(name):
    print(f'任务 {name} 开始执行, 进程号为 {os.getpid()}')
    start_time = time.time()
    time.sleep(random.random() * 3)
    end_time = time.time()
    print(f'任务 {name} 运行了 {end_time - start_time:.2f} 秒')


if __name__ == '__main__':
    print(f'主进程号为 {os.getpid()}')
    # 定义一个进程池
    # processes 是要使用的工作进程数, 若为 None 则使用 os.cpu_count()
    pool = multiprocessing.Pool(processes=3)
    for i in range(8):
        # 每次循环将会用空闲出来的子进程去调用目标
        pool.apply_async(worker, (i,))

    # 关闭 pool, 使其不在接受新的任务
    pool.close()
    # 阻塞当前正在执行的进程(主进程), 等待 pool 中所有子进程执行完成
    # 必须放在 close 语句之后
    pool.join()

# 结果:
# 主进程号为 1718
# 任务 0 开始执行, 进程号为 1719
# 任务 1 开始执行, 进程号为 1720
# 任务 2 开始执行, 进程号为 1721
# 任务 0 运行了 0.55 秒
# 任务 3 开始执行, 进程号为 1719
# 任务 2 运行了 0.56 秒
# 任务 4 开始执行, 进程号为 1721
# 任务 1 运行了 2.06 秒
# 任务 5 开始执行, 进程号为 1720
# 任务 5 运行了 0.11 秒
# 任务 6 开始执行, 进程号为 1720
# 任务 4 运行了 2.10 秒
# 任务 7 开始执行, 进程号为 1721
# 任务 3 运行了 2.86 秒
# 任务 6 运行了 2.36 秒
# 任务 7 运行了 2.38 秒
```

```py
"""
1. pool.apply(func[, args[, kwds]])
使用 args 参数以及 kwds 命名参数调用 func, 它会返回结果前阻塞
这种情况下, apply_async() 更适合并行化工作

2. pool.apply_async(func[, args[, kwds[, callback[, error_callback]]]])
使用参数 args 和关键字参数 kwds 异步调用 func
"""

"""
3. pool.close()
阻止后续任务提交到进程池, 当所有任务执行完成后, 工作进程会退出

4. pool.terminate()
不必等待未完成的任务, 立即停止工作进程
当进程池对象被垃圾回收时, 会立即调用 terminate()

5. pool.join()
等待工作进程结束
调用 join() 前必须先调用 close() 或 terminate()
"""
```

### 4.1. 进程池案例 - 文件夹拷贝器

**进程池中的 Queue** - 实现进程池中进程间通信，需要使用 `multiprocessing.Manager().Queue()` 而不是 `multiprocessing.Queue()`

```py
import os
import multiprocessing


def copy_file(source_dir, dest_dir, file_name, queue):
    """复制文件到指定文件夹"""
    source_path = source_dir + '/' + file_name
    dest_path = dest_dir + '/' + file_name

    with open(source_path, 'rb') as source_file:
        with open(dest_path, 'wb') as dest_file:
            while True:
                data = source_file.read(1024)
                if not data:
                    break
                dest_file.write(data)

    # 向队列中写入复制完成的文件名
    queue.put(file_name)


if __name__ == "__main__":
    # 获取用户要拷贝的目录名
    source_dir = input('请输入要拷贝的目录名: ')
    dest_dir = input('请输入拷贝目标的目录名: ')

    # 创建一个新文件夹
    try:
        os.mkdir(dest_dir)
    except:
        print(f'目录 {dest_dir} 已存在')
    else:
        print(f'目录 {dest_dir} 创建成功')

    # 获取目录内所有要拷贝的文件名
    file_name_list = os.listdir(source_dir)

    # 创建进程池
    pool = multiprocessing.Pool(5)

    # 创建队列
    queue = multiprocessing.Manager().Queue()
    # 进程池创建的进程进行进程通信, 需要使用 Manager().Queue()

    # 向进程池中添加文件复制任务
    for file_name in file_name_list:
        pool.apply_async(copy_file, (source_dir, dest_dir, file_name, queue))

    pool.close()

    # pool.join()
    # 接下来要在主进程实现进度条功能,
    # 当所有文件复制完毕后主进程会自动结束, 所以不需要阻塞主进程

    # 实现进度条提示功能
    file_total = len(file_name_list)  # 要复制的文件数
    success_count = 0  # 成功复制文件数
    while True:
        file_name = queue.get()
        success_count += 1
        # \r 光标回到本行行首
        print(f'\r复制进度 {success_count * 100 / file_total:.2f}%', end='')
        if success_count >= file_total:
            break
    print()

```

## 参考

- [multiprocessing --- 基于进程的并行](https://docs.python.org/zh-cn/3/library/multiprocessing.html#module-multiprocessing)
- [Python 并行编程 中文版](https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/index.html)
